{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_7x7 = np.load('kernels/kernels_7x7.npy').reshape(-1,1,7,7)\n",
    "train_7x7_dataset = utils.TensorDataset(torch.Tensor(train_7x7))\n",
    "train_7x7_loader = utils.DataLoader(train_7x7_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "train_5x5 = np.load('kernels/kernels_5x5.npy').reshape(-1,1,5,5)\n",
    "train_5x5_dataset = utils.TensorDataset(torch.Tensor(train_5x5))\n",
    "train_5x5_loader = utils.DataLoader(train_5x5_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae7x7 = VAE7x7(32,2)\n",
    "optimizer7x7 = optim.Adam(vae7x7.parameters())\n",
    "vae7x7.cuda()\n",
    "\n",
    "vae5x5 = VAE5x5(64,6)\n",
    "optimizer5x5 = optim.Adam(vae5x5.parameters())\n",
    "vae5x5.cuda()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_vae_n_epochs(vae, optimizer, train_loader, n=1):\n",
    "    for epoch in tqdm(range(n)):\n",
    "        train_loss = 0\n",
    "        vae.train()\n",
    "        for batch_idx, (x,) in enumerate(train_loader):\n",
    "            x = x.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            loss = vae.elbo(x, beta=1.)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(loss.item())\n",
    "        #print(train_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_vae_n_epochs(vae7x7, optimizer7x7, train_7x7_loader, n=30*5)\n",
    "torch.save(vae7x7, './models/serialized_vae7x7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vae_n_epochs(vae5x5, optimizer5x5, train_5x5_loader, n=2*5)\n",
    "torch.save(vae5x5, './models/serialized_vae5x5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vae7x7 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(enumerate(train_7x7_loader))[1][0].cuda()\n",
    "z_mean, z_logvar, z, x_mean, x_logvar = vae7x7(x)\n",
    "vae = vae7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=[8,8])\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    t = ax.imshow(x.cpu().data.numpy()[i,0,:,:])\n",
    "    t.set_cmap('RdYlBu')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('figures/original_7x7.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_gen = vae.generate(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=[8,8])\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    t = ax.imshow(xs_gen.cpu().data.numpy()[i,0,:,:])\n",
    "    t.set_cmap('RdYlBu')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('figures/generated_7x7.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vae5x5 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(enumerate(train_5x5_loader))[1][0].cuda()\n",
    "z_mean, z_logvar, z, x_mean, x_logvar = vae5x5(x)\n",
    "vae = vae5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=[8,8])\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    t = ax.imshow(x.cpu().data.numpy()[i,0,:,:])\n",
    "    t.set_cmap('RdYlBu')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('figures/original_5x5.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_gen = vae.generate(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=[8,8])\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    t = ax.imshow(xs_gen.cpu().data.numpy()[i,0,:,:])\n",
    "    t.set_cmap('RdYlBu')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('figures/generated_5x5.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae5x5.eval()\n",
    "vae7x7.eval()\n",
    "for p in vae7x7.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in vae5x5.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_get_accs_every_10_epochs(model, optimizer, n_epochs=101, mode='dwp'):\n",
    "    \n",
    "    list(model.children())[0].mode = mode\n",
    "    list(model.children())[1].mode = mode\n",
    "    \n",
    "    accs = []\n",
    "    for epoch in tqdm(range(1,n_epochs)):      \n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target, reduction='sum') * len(train_loader)\n",
    "            loss += list(model.children())[0].kl(vae7x7) * 1\n",
    "            #loss += list(model.children())[1].kl(vae5x5) * 1\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                    output = model(data)\n",
    "                    test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            \n",
    "            accs.append(100. * correct / len(test_loader.dataset))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * correct / len(test_loader.dataset)))\n",
    "            \n",
    "            \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment On The Max Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sizes = [50,150,500,1000]\n",
    "train_sizes = [50]\n",
    "    \n",
    "accs_for_different_train_sizes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_size in train_sizes:\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.ToTensor()\n",
    "                      ),\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    train_loader.dataset.data = train_loader.dataset.data[:train_size]\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, \n",
    "                       transform=transforms.ToTensor()\n",
    "                      ),\n",
    "        batch_size=128\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model = BayesNet()\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    accs_dwp = train_and_get_accs_every_10_epochs(model,\n",
    "                                                  optimizer,\n",
    "                                                  n_epochs=7001,\n",
    "                                                  mode='dwp')\n",
    "    \n",
    "    \n",
    "    model = BayesNet()\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    accs_gauss = train_and_get_accs_every_10_epochs(model,\n",
    "                                                    optimizer,\n",
    "                                                    mode='gaussian',\n",
    "                                                    n_epochs=7001)\n",
    "    \n",
    "    model = BayesNet()\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    accs_logunif = train_and_get_accs_every_10_epochs(model,\n",
    "                                                    optimizer,\n",
    "                                                    mode='log-uniform',\n",
    "                                                    n_epochs=7001)\n",
    "    \n",
    "    \n",
    "    accs_for_different_train_sizes[train_size] = {'gaussian':accs_gauss,\n",
    "                                                  'dwp':accs_dwp,\n",
    "                                                  'log-uniform':accs_logunif}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=[13, 4])\n",
    "axes = [ax1, ax2]\n",
    "\n",
    "for i,train_size in enumerate(train_sizes[:2]):\n",
    "    ax = axes[i]\n",
    "    ax.plot(accs_for_different_train_sizes[train_size]['gaussian'][:70], label='gaussian')\n",
    "    ax.plot(accs_for_different_train_sizes[train_size]['dwp'][:70], label='dwp')\n",
    "    ax.plot(accs_for_different_train_sizes[train_size]['log-uniform'][:70], label='log-uniform')\n",
    "    ax.set_ylabel('accuracy on validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_title('{} objects in the train dataset'.format(train_size))\n",
    "    ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('figures/small_data.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(layer):\n",
    "    layer.weight.data = torch.nn.init.xavier_uniform_(layer.weight.data)\n",
    "def init_filters(layer, train):\n",
    "    n_filters = layer.weight.shape[0] * layer.weight.shape[1]\n",
    "    inds = np.random.randint(0, train.shape[0], size=n_filters)\n",
    "    new_weight = train[inds].reshape(layer.weight.shape)\n",
    "    layer.weight.data = torch.Tensor(new_weight).cuda()\n",
    "def init_vae(layer, vae):\n",
    "    n_filters = layer.weight.shape[0] * layer.weight.shape[1]\n",
    "    xs_gen = vae.generate(n=n_filters).view(layer.weight.shape)\n",
    "    layer.weight.data = xs_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.ToTensor()\n",
    "                  ),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, \n",
    "                   transform=transforms.ToTensor()\n",
    "                  ),\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get_accs():\n",
    "    accs = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.cross_entropy(output, target) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch_idx % 1 == 0 :\n",
    "\n",
    "            net.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                    output = net(data)\n",
    "                    test_loss += F.cross_entropy(output, target).item()\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            accs.append(100. * correct / len(test_loader.dataset))\n",
    "            \n",
    "            if 100. * correct / len(test_loader.dataset) > 95:\n",
    "                break\n",
    "            \n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs_xavier = []\n",
    "\n",
    "for _ in range(3):\n",
    "    net = SmartInitializedNet()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    net.cuda()\n",
    "\n",
    "    init_xavier(list(net.children())[0])\n",
    "    init_xavier(list(net.children())[1])\n",
    "\n",
    "    accs_xavier.append(train_and_get_accs())\n",
    "    \n",
    "min_len = min([len(accs_xavier[i]) for i in range(3)])\n",
    "for i in range(len(accs_xavier)):\n",
    "    accs_xavier[i] = accs_xavier[i][:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs_filters = []\n",
    "\n",
    "for _ in range(3):\n",
    "    net = SmartInitializedNet()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    net.cuda()\n",
    "\n",
    "    init_filters(list(net.children())[0], train_7x7)\n",
    "    init_filters(list(net.children())[1], train_5x5)\n",
    "\n",
    "    accs_filters.append(train_and_get_accs())\n",
    "    \n",
    "min_len = min([len(accs_filters[i]) for i in range(3)])\n",
    "for i in range(len(accs_filters)):\n",
    "    accs_filters[i] = accs_filters[i][:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs_dwp = []\n",
    "\n",
    "for _ in range(3):\n",
    "    net = SmartInitializedNet()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    net.cuda()\n",
    "\n",
    "    init_vae(list(net.children())[0], vae7x7)\n",
    "    init_vae(list(net.children())[1], vae5x5)\n",
    "\n",
    "    accs_dwp.append(train_and_get_accs())\n",
    "    \n",
    "min_len = min([len(accs_dwp[i]) for i in range(3)])\n",
    "for i in range(len(accs_dwp)):\n",
    "    accs_dwp[i] = accs_dwp[i][:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,7])\n",
    "plt.title('convergence for different initializations. Averaged over multiple runs')\n",
    "plt.plot(np.array(accs_xavier).mean(0), data=None, label='xavier')\n",
    "plt.plot(np.array(accs_filters).mean(0), label='filters')\n",
    "plt.plot(np.array(accs_dwp).mean(0), label='dwp')\n",
    "plt.xlabel('batch id')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('figures/init.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
